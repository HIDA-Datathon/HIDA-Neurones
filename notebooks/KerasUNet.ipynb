{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://idiotdeveloper.com/unet-segmentation-with-pretrained-mobilenetv2-as-encoder/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from src.utils.kerasDataLoader import DataGenerator\n",
    "import src.utils.keras_losses as Loss\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, mobilenet_v2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('models/weights/mobilnetweights.h5'):\n",
    "    MODELPATH = 'models/weights/mobilnetweights.h5'\n",
    "else:\n",
    "    if not os.path.isdir('models/weights/'):\n",
    "        os.mkdir('models/weights/')\n",
    "    mobilenet = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    mobilenet.save_weights('models/weights/mobilenetweights.h5')\n",
    "    MODELPATH = 'models/weights/mobilnetweights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from albumentations import (\n",
    "        HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "        Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "        IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "        IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    "    )\n",
    "\n",
    "    def augmentations(p=0.5):\n",
    "        return Compose([\n",
    "            Flip(),\n",
    "            OneOf([\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=0.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),\n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "        ], p=p)\n",
    "    \n",
    "except:\n",
    "        augmentations = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(image_size, n_classes = 21, MODELPATH=MODELPATH):\n",
    "    inputs = Input(shape=(*image_size, 3), name=MODELPATH)\n",
    "    preproc_layer = tf.keras.layers.Lambda(mobilenet_v2.preprocess_input, name=\"input_image\")(inputs) # Preprocessing function\n",
    "\n",
    "    encoder = MobileNetV2(input_tensor=preproc_layer, weights=None, include_top=False, alpha=1.0)\n",
    "    skip_connection_names = [\"input_image\", \n",
    "                             \"block_1_expand_relu\", \n",
    "                             \"block_3_expand_relu\", \n",
    "                             \"block_6_expand_relu\"]\n",
    "    \n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    \n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(n_classes, (1, 1), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Softmax(axis=-1)(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(image_size=(224,224))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "metrics = [Loss.dice_coef, Recall(), Precision()]\n",
    "model.compile(loss=Loss.FocalLoss, optimizer=opt, metrics=metrics)\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False),\n",
    "    ModelCheckpoint('models/kerasUnet', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c8286935c54ae49c51c296df84f459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d5cf74d2eb450a85de22e4ba905937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(batch_size=16, augmentation=augmentations, preprocessing=None)\n",
    "valid_dataset = DataGenerator(step='valid', shuffle=False, preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0367 - dice_coef: 0.0590 - recall: 6.2327e-07 - precision: 0.0030\n",
      "Epoch 00001: val_loss improved from inf to 0.03272, saving model to models/kerasUnet\n",
      "WARNING:tensorflow:From /home/simon/miniconda3/envs/hida/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/simon/miniconda3/envs/hida/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: models/kerasUnet/assets\n",
      "41/41 [==============================] - 131s 3s/step - loss: 0.0367 - dice_coef: 0.0590 - recall: 6.2327e-07 - precision: 0.0030 - val_loss: 0.0327 - val_dice_coef: 0.0250 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 2/20\n",
      "18/41 [============>.................] - ETA: 1:02 - loss: 0.0307 - dice_coef: 0.0862 - recall: 0.0030 - precision: 0.8273"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python HIDA",
   "language": "python",
   "name": "hida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
