{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/p/software/jusuf/stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# From https://idiotdeveloper.com/unet-segmentation-with-pretrained-mobilenetv2-as-encoder/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "DATAPATH = '../../ufz_im_challenge/photos_annotated'\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from src.utils.kerasDataLoader import DataGenerator\n",
    "import src.utils.keras_losses as Loss\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, mobilenet_v2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "IMAGESIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('models/weights/mobilenetweights.h5'):\n",
    "    MODELPATH = 'models/weights/mobilenetweights.h5'\n",
    "else:\n",
    "    if not os.path.isdir('models/weights/'):\n",
    "        os.makedirs('models/weights/')\n",
    "    mobilenet = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    mobilenet.save_weights('models/weights/mobilenetweights.h5')\n",
    "    MODELPATH = 'models/weights/mobilenetweights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from albumentations import (\n",
    "        HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "        Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "        IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "        IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    "    )\n",
    "\n",
    "    def augmentations(p=0.5):\n",
    "        return Compose([\n",
    "            Flip(),\n",
    "            OneOf([\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=0.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),\n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "        ], p=p)\n",
    "    \n",
    "except:\n",
    "        augmentations = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(image_size, n_classes = 22, MODELPATH=MODELPATH):\n",
    "    inputs = Input(shape=(*image_size, 3), name='input_image')\n",
    "   \n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=MODELPATH, include_top=False, alpha=1.0)\n",
    "    skip_connection_names = [\"input_image\", \n",
    "                             \"block_1_expand_relu\", \n",
    "                             \"block_3_expand_relu\", \n",
    "                             \"block_6_expand_relu\"]\n",
    "    \n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [32, 64, 128, 256]\n",
    "    x = encoder_output\n",
    "    \n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "   # x = tf.keras.layers.Dropout(0.25)(x)\n",
    "        \n",
    "    x = Conv2D(n_classes, (1, 1), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Softmax(axis=-1)(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.utils.keras_losses.BCE_dice(y_true, y_pred)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss.BCE_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(image_size=IMAGESIZE, MODELPATH=None)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "metrics = [Loss.dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]\n",
    "model.compile(loss=Loss.BCE_dice, optimizer=opt, metrics=metrics)\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False, verbose=0),\n",
    "    ModelCheckpoint('models/kerasUnet2', monitor='val_loss', verbose=1, save_best_only=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfedc2dc4bc4b45a4aef53dcad151c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a48d6e4132b4237ac5d4036315f976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(datapath= DATAPATH,  batch_size=32, shuffle=True, augmentation=augmentations, preprocessing=mobilenet_v2.preprocess_input, image_size=IMAGESIZE)\n",
    "valid_dataset = DataGenerator(datapath= DATAPATH, step='valid', shuffle=False, preprocessing=mobilenet_v2.preprocess_input, image_size=IMAGESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 5.6919 - dice_coef: 0.0480 - recall_1: 0.0480 - precision_1: 0.0480\n",
      "\n",
      "Epoch 00001: saving model to models/kerasUnet2\n",
      "20/20 [==============================] - 47s 2s/step - loss: 22.3822 - dice_coef: 0.5673 - recall_1: 0.5288 - precision_1: 0.8180 - val_loss: 5.6919 - val_dice_coef: 0.0480 - val_recall_1: 0.0480 - val_precision_1: 0.0480\n",
      "Epoch 2/120\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 5.6919 - dice_coef: 0.0480 - recall_1: 0.0480 - precision_1: 0.0480\n",
      "\n",
      "Epoch 00002: saving model to models/kerasUnet2\n",
      "20/20 [==============================] - 8s 385ms/step - loss: 17.8364 - dice_coef: 0.6388 - recall_1: 0.6138 - precision_1: 0.8330 - val_loss: 5.6919 - val_dice_coef: 0.0480 - val_recall_1: 0.0480 - val_precision_1: 0.0480\n",
      "Epoch 3/120\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 5.6909 - dice_coef: 0.0480 - recall_1: 0.0480 - precision_1: 0.0480\n",
      "\n",
      "Epoch 00003: saving model to models/kerasUnet2\n",
      "20/20 [==============================] - 8s 386ms/step - loss: 17.0383 - dice_coef: 0.6556 - recall_1: 0.6377 - precision_1: 0.8333 - val_loss: 5.6909 - val_dice_coef: 0.0480 - val_recall_1: 0.0480 - val_precision_1: 0.0480\n",
      "Epoch 4/120\n",
      " 4/20 [=====>........................] - ETA: 4s - loss: 16.4994 - dice_coef: 0.6659 - recall_1: 0.6616 - precision_1: 0.8173"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=120,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DataGenerator(datapath= DATAPATH, step='test', shuffle=False, preprocessing=mobilenet_v2.preprocess_input, image_size=IMAGESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_dataset.images\n",
    "y = test_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_idx = 7\n",
    "plt.subplot(131)\n",
    "plt.imshow(x[viz_idx].squeeze())\n",
    "plt.subplot(132)\n",
    "plt.imshow(np.argmax(pred[viz_idx], -1).squeeze())\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.argmax(y[viz_idx], -1).squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics)\n",
    "model.save('models/final_keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "datathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
