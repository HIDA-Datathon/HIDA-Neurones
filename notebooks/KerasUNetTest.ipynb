{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# From https://idiotdeveloper.com/unet-segmentation-with-pretrained-mobilenetv2-as-encoder/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from src.utils.kerasDataLoader import DataGenerator\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    ")\n",
    "\n",
    "def strong_aug(p=0.5):\n",
    "    return Compose([\n",
    "        RandomRotate90(),\n",
    "        Flip(),\n",
    "        Transpose(),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=0.1),\n",
    "            IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            CLAHE(clip_limit=2),\n",
    "            IAASharpen(),\n",
    "            IAAEmboss(),\n",
    "            RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(p=0.3),\n",
    "    ], p=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(image_size, n_classes = 21):\n",
    "    print((*image_size, 3))\n",
    "    inputs = Input(shape=(*image_size, 3), name=\"input_image\")\n",
    "    \n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n",
    "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(n_classes, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "model = make_model(image_size=(224,224))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "metrics = [dice_coef, Recall(), Precision()]\n",
    "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c22807ff37b48b68ecbdabfa39244ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e59aee25c84c478e62d44145f6c331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading images'), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(batch_size=16, augmentation=strong_aug)\n",
    "valid_dataset = DataGenerator(step='valid', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "41/41 [==============================] - 82s 2s/step - loss: 0.9011 - dice_coef: 0.0989 - recall_2: 0.1277 - precision_2: 0.6287 - val_loss: 0.9333 - val_dice_coef: 0.0667 - val_recall_2: 0.1147 - val_precision_2: 0.3342\n",
      "Epoch 2/20\n",
      "41/41 [==============================] - 82s 2s/step - loss: 0.8257 - dice_coef: 0.1743 - recall_2: 0.2855 - precision_2: 0.5207 - val_loss: 0.8972 - val_dice_coef: 0.1028 - val_recall_2: 0.1992 - val_precision_2: 0.1800\n",
      "Epoch 3/20\n",
      "41/41 [==============================] - 82s 2s/step - loss: 0.7760 - dice_coef: 0.2240 - recall_2: 0.4114 - precision_2: 0.5331 - val_loss: 0.9165 - val_dice_coef: 0.0835 - val_recall_2: 0.1725 - val_precision_2: 0.1842\n",
      "Epoch 4/20\n",
      "41/41 [==============================] - 80s 2s/step - loss: 0.7421 - dice_coef: 0.2579 - recall_2: 0.4708 - precision_2: 0.5339 - val_loss: 0.8932 - val_dice_coef: 0.1068 - val_recall_2: 0.2103 - val_precision_2: 0.1668\n",
      "Epoch 5/20\n",
      "41/41 [==============================] - 81s 2s/step - loss: 0.7107 - dice_coef: 0.2893 - recall_2: 0.5436 - precision_2: 0.5037 - val_loss: 0.9117 - val_dice_coef: 0.0883 - val_recall_2: 0.1929 - val_precision_2: 0.0704\n",
      "Epoch 6/20\n",
      "38/41 [==========================>...] - ETA: 5s - loss: 0.6864 - dice_coef: 0.3136 - recall_2: 0.5729 - precision_2: 0.5036"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python HIDA",
   "language": "python",
   "name": "hida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
